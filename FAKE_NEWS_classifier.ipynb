{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Libraries"
      ],
      "metadata": {
        "id": "oApVOYNG99Lf"
      }
    },
    {
      "source": [
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader punkt\n",
        "!python -m nltk.downloader wordnet"
      ],
      "cell_type": "code",
      "metadata": {
        "outputId": "dbbf0490-4cb1-4c0a-dfb2-72df0976110e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1icD3AZA_WYm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "import seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:43.215753Z",
          "iopub.execute_input": "2022-05-25T23:35:43.216385Z",
          "iopub.status.idle": "2022-05-25T23:35:45.25113Z",
          "shell.execute_reply.started": "2022-05-25T23:35:43.21629Z",
          "shell.execute_reply": "2022-05-25T23:35:45.25025Z"
        },
        "trusted": true,
        "id": "QAjAFoEI99Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z5mWQefpGU9p",
        "outputId": "f1bb064e-66f5-4884-9648-322ddde18b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Data"
      ],
      "metadata": {
        "id": "kdgsBAYX99Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake = pd.read_csv(\"/content/Fake.csv\")\n",
        "df_true = pd.read_csv(\"/content/True.csv\")\n",
        "print(\"Original 'Fake' and 'True' dataframes have the shapes:\", df_fake.shape, \" and \", df_true.shape, \"respectively.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:45.255201Z",
          "iopub.execute_input": "2022-05-25T23:35:45.255507Z",
          "iopub.status.idle": "2022-05-25T23:35:48.262221Z",
          "shell.execute_reply.started": "2022-05-25T23:35:45.255467Z",
          "shell.execute_reply": "2022-05-25T23:35:48.261241Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "0Y_PFlOM99Lf",
        "outputId": "2e75e323-0b0b-428c-8647-3dc5e2f12af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 11600",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e2f41111c6a2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fake.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/True.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original 'Fake' and 'True' dataframes have the shapes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" and \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"respectively.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 11600"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake.head(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:48.264045Z",
          "iopub.execute_input": "2022-05-25T23:35:48.264519Z",
          "iopub.status.idle": "2022-05-25T23:35:48.288959Z",
          "shell.execute_reply.started": "2022-05-25T23:35:48.26445Z",
          "shell.execute_reply": "2022-05-25T23:35:48.287966Z"
        },
        "trusted": true,
        "id": "VyaVaTzF99Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hjQA32qfDnym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inserting a column \"label\" as target feature and Combining DataFrames\n",
        "As there is no specific column for the lebel for False and True texts, we'll add it considering the names of the data sets. Since we are trying to find False text, we'll lable False and True with \"1\" and \"0\", respectively. Later we'll concatenate the 2 data sets and randomly shuffle."
      ],
      "metadata": {
        "id": "ZoBMmNV_99Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake[\"label\"] = \"1\"\n",
        "df_true[\"label\"] = \"0\"\n",
        "df = pd.concat([df_fake, df_true])\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(\"Combined dataframe has shape of \", df.shape)\n",
        "df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:48.305207Z",
          "iopub.execute_input": "2022-05-25T23:35:48.305691Z",
          "iopub.status.idle": "2022-05-25T23:35:48.360411Z",
          "shell.execute_reply.started": "2022-05-25T23:35:48.305654Z",
          "shell.execute_reply": "2022-05-25T23:35:48.359752Z"
        },
        "trusted": true,
        "id": "YAqbD9NB99Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.at[2,'text']"
      ],
      "metadata": {
        "id": "zWLYgofZH9UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keeping necessary columns"
      ],
      "metadata": {
        "id": "warP4-vM99Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To classify the text we will also consider it's title. So we will combine the mentioned columns, and drop 'date' and 'subject' columns."
      ],
      "metadata": {
        "id": "eVt5VLSw99Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.title+df.text\n",
        "df.drop(columns=[\"title\", \"subject\", \"date\"], axis = 1, inplace=True)\n",
        "df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:48.361595Z",
          "iopub.execute_input": "2022-05-25T23:35:48.361978Z",
          "iopub.status.idle": "2022-05-25T23:35:48.548005Z",
          "shell.execute_reply.started": "2022-05-25T23:35:48.361931Z",
          "shell.execute_reply": "2022-05-25T23:35:48.547408Z"
        },
        "trusted": true,
        "id": "PHATJ69W99Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There is no missing data\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:48.548973Z",
          "iopub.execute_input": "2022-05-25T23:35:48.54961Z",
          "iopub.status.idle": "2022-05-25T23:35:48.570121Z",
          "shell.execute_reply.started": "2022-05-25T23:35:48.549568Z",
          "shell.execute_reply": "2022-05-25T23:35:48.569246Z"
        },
        "trusted": true,
        "id": "5jab3MmQ99Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:48.571252Z",
          "iopub.execute_input": "2022-05-25T23:35:48.571842Z",
          "iopub.status.idle": "2022-05-25T23:35:48.579227Z",
          "shell.execute_reply.started": "2022-05-25T23:35:48.57181Z",
          "shell.execute_reply": "2022-05-25T23:35:48.578501Z"
        },
        "trusted": true,
        "id": "BLeB2qed99Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a function to clean and Lemmatize the texts\n",
        "To clean the data we use Lemmatization which is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. In this process we also remove stop-words (useless words such as “the”, “a”, “an”, “in” which don't bring value), and keep only letters in lower-case form."
      ],
      "metadata": {
        "id": "4sk1pwgD99Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def LemmSentence(sentence):\n",
        "    lemma_words = []\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens = word_tokenize(sentence)\n",
        "    for word in word_tokens:\n",
        "        if word not in stop_words:\n",
        "            new_word = re.sub('[^a-zA-Z]', '',word)\n",
        "            new_word = new_word.lower()\n",
        "            new_word = wordnet_lemmatizer.lemmatize(new_word)\n",
        "            lemma_words.append(new_word)\n",
        "    return \" \".join(lemma_words)\n",
        "\n",
        "X = [LemmSentence(i) for i in X]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:35:48.580496Z",
          "iopub.execute_input": "2022-05-25T23:35:48.580703Z",
          "iopub.status.idle": "2022-05-25T23:41:40.803588Z",
          "shell.execute_reply.started": "2022-05-25T23:35:48.580677Z",
          "shell.execute_reply": "2022-05-25T23:41:40.802463Z"
        },
        "trusted": true,
        "id": "1169PhQQ99Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining dependent and independent variables and training the sets"
      ],
      "metadata": {
        "id": "kaIYMx5C99Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:40.80612Z",
          "iopub.execute_input": "2022-05-25T23:41:40.806402Z",
          "iopub.status.idle": "2022-05-25T23:41:40.821632Z",
          "shell.execute_reply.started": "2022-05-25T23:41:40.806371Z",
          "shell.execute_reply": "2022-05-25T23:41:40.820698Z"
        },
        "trusted": true,
        "id": "IzIC18ga99Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=7)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:40.823008Z",
          "iopub.execute_input": "2022-05-25T23:41:40.823704Z",
          "iopub.status.idle": "2022-05-25T23:41:40.839548Z",
          "shell.execute_reply.started": "2022-05-25T23:41:40.823667Z",
          "shell.execute_reply": "2022-05-25T23:41:40.83878Z"
        },
        "trusted": true,
        "id": "v3cW9wFo99Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting text to vectors using Tfidf vectorizer\n",
        "\n",
        "Term Frequency (TF) = (Frequency of a term in the document)/(Total number of terms in documents)\n",
        "\n",
        "Inverse Document Frequency(IDF) = log( (total number of documents)/(number of documents with term t))"
      ],
      "metadata": {
        "id": "xRskW1uC99Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# transforming\n",
        "tfidf_train = vectorizer.fit_transform(x_train.iloc[:,0])\n",
        "tfidf_test = vectorizer.transform(x_test.iloc[:,0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:40.840797Z",
          "iopub.execute_input": "2022-05-25T23:41:40.841467Z",
          "iopub.status.idle": "2022-05-25T23:41:54.815681Z",
          "shell.execute_reply.started": "2022-05-25T23:41:40.841417Z",
          "shell.execute_reply": "2022-05-25T23:41:54.814831Z"
        },
        "trusted": true,
        "id": "xsXyiKFX99Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_train.shape, tfidf_test.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:54.817142Z",
          "iopub.execute_input": "2022-05-25T23:41:54.81749Z",
          "iopub.status.idle": "2022-05-25T23:41:54.824651Z",
          "shell.execute_reply.started": "2022-05-25T23:41:54.817449Z",
          "shell.execute_reply": "2022-05-25T23:41:54.823564Z"
        },
        "trusted": true,
        "id": "WhW27n8699Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying PassiveAggressiveClassifier and visualizing heatmap for Confusion matrix\n",
        "\n",
        "Passive-Aggressive algorithms are called so because :\n",
        "\n",
        "Passive: If the prediction is correct, keep the model and do not make any changes. i.e., the data in the example is not enough to cause any changes in the model.\n",
        "\n",
        "Aggressive: If the prediction is incorrect, make changes to the model. i.e., some change to the model may correct it.\n",
        "\n",
        "C : This is the regularization parameter, and denotes the penalization the model will make on an incorrect prediction\n",
        "\n",
        "max_iter : The maximum number of iterations the model makes over the training data.\n",
        "\n",
        "tol : The stopping criterion. If it is set to None, the model will stop when (loss > previous_loss  –  tol). By default, it is set to 1e-3."
      ],
      "metadata": {
        "id": "kaBO2wEY99Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pac = PassiveAggressiveClassifier(random_state = 7,loss = 'squared_hinge',  max_iter = 50, C = 0.16)\n",
        "pac.fit(tfidf_train, y_train.values.ravel())\n",
        "\n",
        "#Predict on the test set and calculate accuracy\n",
        "y_pred = pac.predict(tfidf_test)\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {round(score*100, 2)}%')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:54.826127Z",
          "iopub.execute_input": "2022-05-25T23:41:54.826514Z",
          "iopub.status.idle": "2022-05-25T23:41:55.527642Z",
          "shell.execute_reply.started": "2022-05-25T23:41:54.826471Z",
          "shell.execute_reply": "2022-05-25T23:41:55.52683Z"
        },
        "trusted": true,
        "id": "Hub7HhTA99Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's visualize the result with Confusion Matrix in terms of True Positive, False Positive, True Negative, False Negative."
      ],
      "metadata": {
        "id": "AO5j0s_a99Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt=\"d\")\n",
        "ax.set(xlabel='Prediction', ylabel='Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:55.528843Z",
          "iopub.execute_input": "2022-05-25T23:41:55.52908Z",
          "iopub.status.idle": "2022-05-25T23:41:55.817036Z",
          "shell.execute_reply.started": "2022-05-25T23:41:55.529051Z",
          "shell.execute_reply": "2022-05-25T23:41:55.815965Z"
        },
        "trusted": true,
        "id": "RxD3ARf999Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More frequent words in WordClound"
      ],
      "metadata": {
        "id": "dsi9h-nJ99Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "word = X[0] #the first news\n",
        "wc = WordCloud(background_color=\"black\", max_words=3000, max_font_size=256,\n",
        "               random_state=13, width=1500, height=1500, prefer_horizontal=0.5)\n",
        "wc.generate(' '.join(word))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-25T23:41:55.818536Z",
          "iopub.execute_input": "2022-05-25T23:41:55.818853Z",
          "iopub.status.idle": "2022-05-25T23:44:07.317412Z",
          "shell.execute_reply.started": "2022-05-25T23:41:55.81881Z",
          "shell.execute_reply": "2022-05-25T23:44:07.316564Z"
        },
        "trusted": true,
        "id": "_s9LsP9d99Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking the model with random Text"
      ],
      "metadata": {
        "id": "u5t43E6U99Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_lable(n):\n",
        "    if n == '1':\n",
        "        return \"\\n\\tFake News ((((\"\n",
        "    if n == '0':\n",
        "        return \"\\n\\tNot A Fake News))))\"\n",
        "\n",
        "def manual_testing(news):\n",
        "    lnews = LemmSentence(news)\n",
        "\n",
        "    df = pd.DataFrame([lnews])\n",
        "\n",
        "    x = df.iloc[:,0]\n",
        "    x = vectorizer.transform(x)\n",
        "\n",
        "    x_pred = pac.predict(x)\n",
        "\n",
        "    return output_lable(x_pred)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-26T01:13:55.404859Z",
          "iopub.execute_input": "2022-05-26T01:13:55.405174Z",
          "iopub.status.idle": "2022-05-26T01:13:55.411349Z",
          "shell.execute_reply.started": "2022-05-26T01:13:55.40514Z",
          "shell.execute_reply": "2022-05-26T01:13:55.410739Z"
        },
        "trusted": true,
        "id": "sWtn-JiL99Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = 'Turkish hunger striker found guilty of militant links, freedISTANBUL (Reuters) - A Turkish professor who has been on hunger strike since losing her job in a purge following last year s failed coup was convicted on Friday of belonging to a banned far-left group but the court ordered her release pending an appeal. Nuriye Gulmen, 35, was sentenced to six years and three months in jail for being a member of the militant leftist DHKP-C group, deemed a terrorist organization by Turkey, defense lawyers told Reuters. She was found not guilty of lesser charges including organizing illegal rallies.  The literature professor had been hospitalized before the trial began due to her worsening health after seven months of surviving on water, herbal tea and sugar and salt solutions. Primary school teacher Semih Ozakca, 28, who has also been on hunger strike since losing his job in the crackdown, was acquitted on similar charges. The Ankara court had ordered his release on Oct. 21 for the remainder of the trial, on condition that he wear an ankle monitor. Both deny any links to DHKP-C. A third defendant, Acun Karadag, was acquitted on a lesser charge of participating in illegal rallies.  The teachers have said their hunger strike aimed to highlight the plight of some 150,000 state employees   including academics, civil servants, judges and soldiers   suspended or sacked since the abortive coup in July 2016. The pair were detained in May and jailed pending the start of the trial in September. On Sept. 12, days before the teachers were due in court, Turkey issued detention warrants for the lawyers who were set to defend them. Turkish authorities blame the coup attempt on U.S.-based Muslim cleric Fethullah Gulen and his supporters. Gulen condemned the coup and denies involvement. Human rights groups and the European Union have said President Tayyip Erdogan is using the crackdown to stifle dissent in Turkey, an assertion that he denies.'\n",
        "print(manual_testing(news))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-26T01:14:37.835159Z",
          "iopub.execute_input": "2022-05-26T01:14:37.835467Z",
          "iopub.status.idle": "2022-05-26T01:14:40.210451Z",
          "shell.execute_reply.started": "2022-05-26T01:14:37.835436Z",
          "shell.execute_reply": "2022-05-26T01:14:40.209436Z"
        },
        "trusted": true,
        "id": "q9UDc3bI99Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I hope you found this post interesting and useful.\n",
        "\n",
        "if so, please upvote. Your **upvote** is highly **appreciated**. And any comments are always welcome)))\n",
        "\n",
        "Thanks!"
      ],
      "metadata": {
        "id": "I6zqBR9O99Lj"
      }
    }
  ]
}